import torch,nltk,matplotlib.pyplot as plt; from nltk.tokenize import word_tokenize; from tokenizers import Tokenizer; from tokenizers.models import BPE; from tokenizers.trainers import BpeTrainer; from tokenizers.pre_tokenizers import Whitespace; from transformers import AutoTokenizer,AutoModel; from sklearn.decomposition import PCA
nltk.download("punkt")
nltk.download("punkt_tab")

text="Tokenization techniques are crucial for large language models"
word_tokens=word_tokenize(text.lower())
char_tokens=list(text.lower())

bpe_tok=Tokenizer(BPE()); bpe_tok.pre_tokenizer=Whitespace()
bpe_tok.train_from_iterator([text],BpeTrainer(vocab_size=50,special_tokens=["[UNK]"]))
bpe_tokens=bpe_tok.encode(text).tokens

wp_tok=AutoTokenizer.from_pretrained("bert-base-uncased")
wp_tokens=wp_tok.tokenize(text)

model=AutoModel.from_pretrained("bert-base-uncased"); model.eval()
inputs=wp_tok(text,return_tensors="pt")
with torch.no_grad(): outputs=model(**inputs)

token_emb=outputs.last_hidden_state; cls_emb=token_emb[:,0,:]; mean_emb=token_emb.mean(dim=1)

print("WORD:",word_tokens)
print("CHAR:",char_tokens)
print("BPE:",bpe_tokens)
print("WORDPIECE:",wp_tokens)
print("SHAPES:",token_emb.shape,cls_emb.shape,mean_emb.shape)

pca=PCA(n_components=2); reduced=pca.fit_transform(token_emb.squeeze(0).numpy())
plt.scatter(reduced[:,0],reduced[:,1])
[plt.annotate(tok,(reduced[i,0],reduced[i,1])) for i,tok in enumerate(wp_tokens)]
plt.title("WordPiece Embeddings (BERT)"); plt.show()
